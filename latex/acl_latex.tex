% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{amsmath}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Enhancing Multilingual Reasoning in LLMs: \\
Insights from Cross-Linguistic Correlations and Optimal Data Proportions}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) typically rely on fine-tuning to enhance their reasoning capabilities across various languages. However, limited research has been conducted on the optimal balance of language proportions within multilingual reasoning datasets. To fill this gap, we performed a systematic study to examine how different proportions of language data in multilingual reasoning datasets influence fine-tuning performance. Our study revealed a clear relationship between language proportions in datasets and the fine-tuning performance of LLMs. By fine-tuning multiple LLMs using the appropriate language distributions and data volumes identified in our study, we achieved outstanding performance in both multilingual mathematical reasoning and solving mathematical problems using Python code. Furthermore, our approach significantly reduced data volume requirements and translation costs compared to existing methods, providing a valuable reference for future research.
\end{abstract}

\section{Introduction}

Despite recent significant advancements in large language models (LLMs) ~\citep{openai2023gpt4,chowdhery2023palm,touvron2023llama,brown2020languagemodelsfewshotlearners,workshop2023bloom176bparameteropenaccessmultilingual}, LLMs still encounter considerable challenges in reasoning tasks for non-English languages, particularly low-resource languages ~\citep{shi2022language,huang2023not,qin2023cross}. A critical research question thus emerges: how can LLMs, primarily trained on high-resource languages, effectively generalize their reasoning capabilities to low-resource languages with insufficient data? 

Fine-tuning has been the commonly adopted solution to this issue. Prior studies have primarily focused on fine-tuning models using translated mathematical reasoning datasets across multiple languages. For instance, one approach enhanced LLMs’ multilingual reasoning capabilities using translated datasets ~\citep{chen2023breakinglanguagebarriersmultilingual}, while another improved model performance through question alignment ~\citep{zhu2024question,zhu2024power}.

While these methods have improved multilingual reasoning performance, they still exhibit significant limitations. First, previous work often involved translating English datasets into multiple languages in equal proportions for fine-tuning. This approach is costly, overlooks the impact of language proportions in the fine-tuning datasets. Second, in scenarios requiring extensive multilingual data, it is impractical to translate English data into multiple languages in equal amounts. Thus, the key is to efficiently leverage a small amount of low-resource language data to broadly enhance the multilingual reasoning capabilities of LLMs.

Moreover, earlier studies mainly focused on a limited number of languages and rarely tested generalization capabilities across more than 20 languages. Additionally, most prior research was conducted on only a few LLMs, and it remains unclear whether these approaches generalize well to other, more advanced models.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\linewidth]{picture/figure1_pie.png}
\end{center}
\caption{Two charts represent the proportion
of data for different languages in the multilingual mathematical reasoning dataset. In previous methods, the amount of data for each language was the same (as shown in the left chart).
Through our research, we determined the optimal proportion for each language (as shown in the right chart).}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\linewidth]{picture/figure_1_data.png}
\end{center}
\caption{We use this data format to teach model to solve mathematical reasoning task with executable code.}
\end{figure}

In this paper, we address these challenges through a systematic investigation of how varying language proportions in multilingual mathematical reasoning datasets affect fine-tuning outcomes, and determine the optimal amount of data needed for fine-tuning, as illustrated in Figure 1.Directly exploring the optimal ratio among tens of languages is extremely challenging, as it requires determining the optimal proportions among tens of variables, which results in an extremely large search space. So we adopted an innovative approach to effectively reduce the search space and determined the appropriate ratio for the 10 languages commonly used in previous works ~\citep{zhu2024question,zhu2024power}, which was further extended to 25 languages. Additionally, we explored the impact of data volume on the fine-tuning performance of LLMs. Ultimately, building on previous research, we created the largest multilingual mathematical reasoning dataset, HighMath-350k, alongside a multilingual code reasoning dataset(Figure 2), HighCode-350k. Fine-tuning multiple LLMs on these datasets resulted in state-of-the-art performance, clearly demonstrating the efficacy of our approach.

In summary, our key contributions are as follows:

\begin{itemize}
    \item Through more than 600 groups of experiments, we analyzed and modeled how the proportions of different languages in multilingual reasoning datasets affect fine-tuning performance, and determined the appropriate volume of fine-tuning data.
    \item We constructed  multilingual chain-of-thought mathematical reasoning dataset, HighMath-350k, as well as the multilingual code reasoning dataset, HighCode-350k.
    \item Based on multiple LLMs, we trained state-of-the-art multilingual chain-of-thought reasoning models and code reasoning models.
\end{itemize}


\section{Related Work}

\subsection{Multilingual Mathematical Reasoning}
Mathematical reasoning is a core task for assessing the intelligence of LLMs ~\citep{zhang2024mathverse,zhang2024mario}, requiring them to understand mathematical problems and generate answers through step-by-step reasoning~\citep{ahn2024largelanguagemodelsmathematical,zhang2022automatic,liu2023federated}. \citep{shi2022language} expanded this evaluation to a multilingual setting by translating English math questions from the GSM8K test set~\citep{cobbe2021trainingverifierssolvemath} into various non-English languages, introducing the multilingual benchmark MGSM.


\begin{table*}[h]
\centering
\caption{Base LLMs used in different phases}
\begin{tabular}{|c|l|}
\hline
\textbf{Phase} & \textbf{Base LLMs} \\ \hline
Phase 1 & LLaMA3.1-8B, LLaMA3-8B, Phi3.5-mini-instruct, Mistral-7B-v0.3, Qwen2-7B \\ \hline
Phase 2 & LLaMA3.1-8B, LLaMA3-8B, Phi3.5-mini-instruct, Mistral-7B-v0.3, Qwen2-7B \\ \hline
Phase 3 & LLaMA3-8B, LLaMA3-70B \\ \hline
\end{tabular}
\end{table*}
\begin{table*}[h]
\centering
\caption{Experimental languages used in different phases}
\begin{tabular}{|c|l|}
\hline
\textbf{Phase} & \textbf{Experimental Languages} \\ \hline
Phase 1 & en, ru, de, fr, es, ja, zh, sw, th, bn, lt, cs, ka, ar \\ \hline
Phase 2 & en, ru, de, fr, es, ja, zh, sw, th, bn \\ \hline
Phase 3 & zh, en, es, hi, ar, pt, bn, ru, ja, pa, jv, de, ko, fr, te, vi, tr, ta, it, fa, ur, mr, sw, th, pl \\ \hline
\end{tabular}
\end{table*}


Efforts to improve LLMs' multilingual reasoning performance have been continuing.For example, ~\citep{huang2023languagescreatedequalllms} and ~\citep{qin-etal-2023-cross} explored prompting ChatGPT~\citep{openai2023gpt4} to translate non-English questions into English and generate answers based on the translations. However, ~\citep{hu2024largelanguagemodelscrosslingual} found that this prompting strategy is not consistently effective for open-source LLMs. To enhance the multilingual capabilities of these models, researchers like ~\citep{nguyen2024seallmslargelanguage} have explored continued pretraining on large-scale non-English corpora, though this approach is resource-intensive and data-inefficient.

\subsection{LLM's Language Preference}
LLMs, with their large-scale parameters~\citep{aki1967scaling,huo2009scaling,wang2006scaling,rosenfeld1999quasi}, pre-trained on vast corpora and fine-tuned on comprehensive instruction datasets~\citep{huang2016well,zhao2020ape210k,lindstrom2022clevr,koncel2016mawps}, have demonstrated impressive intelligence~\citep{touvron2023llama,floridi2020gpt,kalyan2023survey,lagler2013gpt2}. However, empirical studies show that LLMs still struggle with multilingual scenarios, particularly for low-resource languages~\citep{shi2022languagemodelsmultilingualchainofthought, zhu2024question,weyssow2024codeultrafeedback}. This issue is mainly due to the dominance of English in both pretraining~\citep{blevins2022languagecontaminationhelpsexplain} and instruction datasets~\citep{wang2023farcamelsgoexploring}. In this work, we focus on a core capability of LLMs—their reasoning ability—and aim to push the boundaries of their performance in multilingual reasoning tasks.


\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{picture/figure_1_diff.png}
\end{center}
\caption{}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=1\linewidth]{picture/figure_2.png}
\end{center}
\caption{The score in the image represents the average of the LLM's scores across the 10 languages.}
\end{figure*}


\begin{figure*}[h]
    \centering
    
    % 第一行的三个子图
    \begin{subfigure}[b]{0.3\linewidth}  % 每个子图宽度为整个图像宽度的30%
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_LLaMA3.1.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_Phi3.5-mini.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_LLaMA3-8B.png}
    \end{subfigure}
    
    % 第二行的三个子图
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_Mistral-7B-v0.3.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_Qwen2-7B.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure3_Average.png}
    \end{subfigure}
    
    \caption{The relative proportion of English is fixed at 24. The horizontal axis represents the relative proportion of the second group of languages, while the vertical axis represents the relative proportion of the third group of languages. The results are averaged across the 10 languages.}
\end{figure*}


\begin{figure*}[h]
    \centering
   
    % 第一行的三个子图
    \begin{subfigure}[b]{0.3\linewidth}  % 每个子图宽度为整个图像宽度的30%
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_llama3.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_qwen.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_llama.png}
    \end{subfigure}
    
    % 第二行的三个子图
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_mistral.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_phi.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_average.png}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure4_text.png}
    \end{subfigure}
    
    \caption{The proportions in the figure are represented as en:ru:de:fr:es:ja:zh:bn:sw:th
in sequence.}
\end{figure*}

\begin{figure*}[h]
    \centering
   
    % 第一行的三个子图
    \begin{subfigure}[b]{0.3\linewidth}  % 每个子图宽度为整个图像宽度的30%
        \centering
        \includegraphics[width=\linewidth]{picture/figure5_8b_10lang.png}
        \caption{LLaMA3-8B based on 10 languages}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure5_8b_25lang.png}
        \caption{LLaMA3-8B based on 25 languages}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{picture/figure5_70b_25lang.png}
        \caption{LLaMA3-70B based on 25 languages}
    \end{subfigure}
    \caption{}
\end{figure*}


\begin{figure*}[h]
\begin{center}
\includegraphics[width=\linewidth]{picture/figure6_sota.png}
\end{center}
\caption{Evaluation results on the MGSM dataset. The score in the image represents the average of the LLM's scores across the 10 languages.}
\end{figure*}


\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/figure7_msvamp_llama2.png} 
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/figure7_MSVAMP.png} 
        \label{fig:subfig2}
    \end{subfigure}
    \caption{In the left image, the baseline represents ~\citep{zhu2024power}, and non-English refers to the average performance across 9 non-English languages. In the right image, the baseline represents the fine-tuning results using an equal amount of data, but with the proportion of different languages in the dataset being balanced. The scores are averaged across 10 languages.}
    \label{fig:overall}
\end{figure*}


\begin{figure*}[h]
\begin{center}
\includegraphics[width=0.8\linewidth]{picture/figure8_code.png}
\end{center}
\caption{The score in the image represents the average of the LLM's scores across the 10 languages.}
\end{figure*}


\section{Methodology}
This section describes the systematic method used to determine the appropriate language proportions in the dataset and the required data volume for effective fine-tuning. Our method is divided into three phases, as detailed below, with the experimental setup provided in Section 4.1.

\subsection{Phase One}
Directly determining the optimal ratio among 10 languages is also unrealistic, as directly exploring the optimal proportions among 10 variables would result in a search space of about \(10^{10}\).To address this issue and better generalize the reasoning capabilities of LLMs to different language contexts, we first examined the correlation between LLMs' alignment with English and other languages, and their mathematical reasoning capabilities. Through this experiment, we aim to reduce the search space to an acceptable size.

 This alignment was assessed through a Non-English to English translation task. First, several LLMs were fine-tuned on the English mathematical reasoning dataset, MetaMathQA-395k~\citep{yu2024metamath}, to evaluate their translation capabilities from Non-English to English, both before and after fine-tuning. These models were then fine-tuned on translation data from non-English to English, and their performance on multilingual mathematical reasoning tasks was evaluated.Our results showed a positive correlation between LLMs' reasoning abilities and their alignment between English and French, German, and Russian, while no significant correlation was found with other languages.This finding will greatly help the subsequent experiments.


\subsection{Phase Two}
Building on the insights from Phase One, we constructed a multilingual mathematical reasoning dataset encompassing 10 languages and grouped these 10 languages into three groups(as shown in Table 3) based on the findings from Phase One, treating them as three variables for further exploration. By systematically varying the proportions of data for each group, we utilized five models and conducted over 600 experiments to identify the optimal distribution of three groups. The findings indicate that, given a fixed total data volume, the optimal language ratio for enhancing the multilingual reasoning capabilities of LLMs is group1:group2:group3 = 4:4:1 (en:ru:de:fr:es:ja:zh:bn:sw
= 24:8:8:8:1:1:1:1:1:1). Additionally, we developed a mathematical model to elucidate the relationship between data proportions and fine-tuning outcomes, and evaluated the contribution of each language to the overall improvement of multilingual reasoning.


\subsection{Phase Three}
To determine the appropriate volume of fine-tuning data, we first conducted experiments with 10 languages and later extended the study to 25 languages. We tested varying data volumes and compared the results with previous approaches that used equal data volumes across languages. The experiments included fine-tuning data volumes up to 9.875M and scaling model parameters to 70B, demonstrating the clear advantages of our proposed methodology.


\section{Experiment}

\subsection{Optimal Proportion and Data Volume Selection for Multilingual Fine-Tuning}
This section describes the experimental setup and key results for the three phases outlined in the methodology.

\subsubsection{Experimental Setup} 
\textbf{Base LLMs}

Different base LLMs were selected for each phase based on the research objectives. The specific models are listed in Table 1.

\textbf{Experimental Languages}

The selection of test languages for each phase was guided by specific research objectives and computational constraints. In Phase One, the objective was to investigate the relationship between alignment with English and the mathematical reasoning capabilities of various non-English languages. To achieve this, We primarily selected several high-resource non-English languages. Phase Two employed the 10 languages from the MGSM dataset, while Phase Three aimed to assess the generalizability of our approach across 25 widely spoken languages. The languages chosen for each phase are detailed in Table 2.



\textbf{Translation Strategy}

We translated both the questions and answers in the multilingual chain-of-thought mathematical reasoning dataset. To ensure that the generated multilingual data effectively enhanced the model's reasoning capabilities, we used the powerful open-source model DeepSeek-Chat-v2-236B ~\citep{deepseekv2} for high-quality translations, adjusting its hyperparameters for this task. To maintain translation quality and consistency, we implemented the following strategies:
\begin{itemize}
\item Mathematical formulas were preserved, and all numbers were converted to Arabic numerals to facilitate cross-linguistic prediction.
\item To improve translation accuracy, we included two examples in the prompts for each language.
\end{itemize}
For the mathematical reasoning with code dataset, we only translated the question parts since the solution involves Python code.

To ensure consistency across languages, we extracted all mathematical expressions from the translated questions and answers. If the calculations were correct and matched the English version, the translation was deemed accurate. If errors persisted across five consecutive translations, the corresponding data was discarded.

This approach ensures coherence and accuracy in the translation process, enabling comprehensive evaluation and application of the dataset in a multilingual context, while maintaining both linguistic and mathematical integrity.

\textbf{Tasks and Datasets}

In terms of test task selection,in Phase One, we used a translation task to assess the alignment between English and non-English languages within the LLM. In the subsequent phases, chain-of-thought mathematical reasoning tasks were used to evaluate the LLM’s reasoning abilities.

For the training datasets, Phase One used the MetaMathQA-395k and WMT datasets. In later phases, we translated the 395k entries from MetaMathQA into 25 languages, generating a total of 9.875M entries. In Phase Two, 200k entries from each language were selected, totaling 1M entries. Phase Three used the full 9.875M dataset.

For the test datasets, Phase One used the MGSM dataset to evaluate the LLMs' multilingual mathematical reasoning capabilities, and 3,000 entries from the WMT dataset were selected to assess translation performance. In the subsequent phases, the MGSM dataset was used as the primary test dataset.


\subsubsection{main results}

\textbf{Phase 1}

Figure 3 illustrates the changes in translation performance across different languages before and after fine-tuning the LLMs on the MetaMathQA-395k dataset. Significant improvements were observed for German, Russian, and French, while performance in other languages remained mostly unchanged.

Figure 4 presents the model's performance on the MGSM dataset after fine-tuning with different non-English to English translation pairs. Since no additional measures were taken to enhance reasoning ability, mathematical reasoning performance declined post-fine-tuning. However, German, Russian, and French still outperformed other languages in reasoning tasks.

These results suggest a positive correlation between the LLM's reasoning ability and its alignment with languages like French, German, and Russian.

\begin{table}[h]
\centering
\caption{Language Groups for the Experiment}
\begin{tabular}{|c|l|}
\hline
\textbf{Group} & \textbf{Languages} \\ \hline
Group 1        & en                \\ \hline
Group 2        & de, ru, fr         \\ \hline
Group 3        & es, ja, zh, th, bn, sw \\ \hline
\end{tabular}
\end{table}



\textbf{Phase 2}

Based on the conclusions from the first phase, we divided the 10 languages selected for the second phase into three groups, as shown in Table 3. In each group, the amount of mathematical reasoning data for each language was kept consistent. By adjusting the overall data ratio among these three groups, we explored the optimal distribution of languages. 
As shown in Figure 5, although the pretraining corpora for each model differ, all five models demonstrated optimal performance when the ratio of the three language groups was around 24:24:6.(4:4:1) 

To refine the optimal data ratio, we conducted further experiments to assess the relative importance of languages in the second and third groups. As shown in Figure 6, increasing the proportion of a single language did not lead to any clear improvement in prediction accuracy. Even when improvement occurred, the gains were minimal.

\textbf{Mathematical Modeling}

To better model the patterns observed in Figure 5 and provide suggestions for data allocation in future studies, we used the Gaussian Process Regression method (GPR). GPR is employed to model the underlying function \(f(x)\) from the observed data, assuming that any finite set of function values follows a multivariate Gaussian distribution. The model can be expressed as:

\[
\mathbf{y} = f(X) + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I),
\]

where \(X\) represents the input features, \(\mathbf{y}\) is the observed output, and \(\epsilon\) is Gaussian noise with variance \(\sigma^2\).

We experimented with various mathematical models and computational methods, with detailed analyses provided in the appendix. Ultimately, we finalized the following kernel:

\[
k_{\text{RQ}}(X, X') = \sigma_f^2 \left(1 + \frac{\| X - X' \|^2}{2 \alpha l^2} \right)^{-\alpha}
\]

\textbf{Phase 3}

This phase of the experiment was divided into two groups. The first group used the original 10 languages to construct different sizes of the multilingual mathematical reasoning dataset, HighMath, with a language ratio of en:ru:de:fr:es:ja:zh:bn:sw:th = 24:8:8:8:1:1:1:1:1:1. The second group included 25 languages and constructed different sizes of the HighMath dataset, using a ratio of en:de:ru:fr:others = 24:8:8:8:12.

We conducted a series of experiments, with the results shown in Figure 7. HighMath-350k and HighMath-1.5M was identified as a relatively optimal size. 



\subsection{Model Evaluation}
In this section, we fine-tune several LLMs on the HighMath-350k dataset and compare their performance against baseline models. We also extend our approach to solving mathematical problems using python code, demonstrating the generalizability and effectiveness of our method.

\subsubsection{Mathematical Reasoning}

\textbf{Baseline} For the MGSM dataset, we selected the most advanced fine-tuned baseline available ~\citep{zhu2024question,zhu2024power}. For the MSVAMP evaluation dataset, we used multiple baselines, including one derived from fine-tuning LLaMA2 series models ~\citep{zhu2024question,zhu2024power}. We also evaluated other LLMs; however, due to the lack of directly comparable baselines, we used a dataset with 350k samples, evenly distributed across 10 languages, as an additional baseline.

\textbf{Main Results} 
The results, shown in Figures 8 and 9, indicate that the models fine-tuned on the HighMath-350k dataset not only surpassed the baselines on the MGSM evaluation dataset, but also outperformed baseline models with larger parameter sizes from the LLaMA3 and Mistral series. The models also performed well on the MSVAMP evaluation dataset. Moreover, the amount of training data used was only about 70\% of that used for the baseline models, highlighting the efficiency of our approach.

\subsubsection{Solving Mathematical Reasoning Tasks Using Python Code}
Using the HighCode dataset, our models continued to outperform the baseline ~\citep{zhu2024power} across multiple base models in tasks involving reasoning with Python code, as shown in Figure 10. This demonstrates the strong generalizability of our approach.



\section{Conclusion}

In this paper, we propose an innovative three-phase approach and conduct extensive experiments to investigate the impact of different language data proportions in multilingual training datasets on LLM fine-tuning for multilingual mathematical reasoning tasks. Additionally, we establish a mathematical model to illustrate the relationship between these factors. We also perform large-scale experiments to explore the correlation between fine-tuning effectiveness and the amount of training data in multilingual mathematical reasoning datasets. Using an optimal data scale, we construct the multilingual chain-of-thought reasoning dataset HighMath-350k and the HighCode-350k dataset, designed for solving mathematical problems using Python code in a multilingual context. Leveraging these datasets, we fine-tuned several LLMs to achieve outstanding performance, providing valuable insights for future research.



\clearpage
\section*{Limitation}

This paper has conducted an in-depth exploration only on multilingual mathematical reasoning tasks. However, it has not investigated whether the performance of LLMs on other multilingual tasks is also related to the data distribution in the fine-tuning datasets, or whether similar methods can be used to improve the performance of large models in other multilingual tasks.

This paper explores the data distribution of different languages in the dataset for multilingual mathematical reasoning tasks through extensive experiments and identifies the patterns exhibited. However, it does not analyze the internal mechanisms of the LLMs. Providing an explanation for this phenomenon from the perspective of the interpretability of LLMs would be an interesting topic for further research.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}


% \clearpage
\appendix
\onecolumn

% \section{Example Appendix}
% \label{sec:appendix}

\section*{Appendix: Gaussian Process Regression Model Fitting and Evaluation}

\subsection*{Introduction to Gaussian Process Regression}

\section{Gaussian Process Regression: Mathematical Foundation and Kernel Selection}

\subsection{Mathematical Foundation of Gaussian Process Regression}

Gaussian Process Regression (GPR) is a non-parametric Bayesian method used for regression problems. It assumes that the observed data can be generated by a Gaussian process, which is a collection of random variables such that any finite subset has a joint Gaussian distribution.

Given a training dataset $\{(X_i, y_i)\}_{i=1}^n$, where $X_i$ represents the input variables (in this case, two-dimensional coordinates) and $y_i$ represents the corresponding observations, we assume:

\[
y_i = f(X_i) + \varepsilon_i
\]

where $f(X)$ is the latent true function, and $\varepsilon_i \sim \mathcal{N}(0, \sigma_n^2)$ is independent and identically distributed Gaussian noise.

A Gaussian process prior is placed on $f(X)$:

\[
f(X) \sim \mathcal{GP}(m(X), k(X, X'))
\]

where $m(X)$ is the mean function, typically set to zero, $m(X) = 0$, and $k(X, X')$ is the covariance function (kernel) that measures the similarity between input points.

\subsubsection{Predictive Distribution}

For a new input point $X_*$, the predictive distribution is also a Gaussian distribution with mean and variance given by:

\[
\mu_* = k_*^\top (K + \sigma_n^2 I)^{-1} y, \quad \sigma_*^2 = k(X_*, X_*) - k_*^\top (K + \sigma_n^2 I)^{-1} k_*
\]

where:

\begin{itemize}
    \item $y = [y_1, y_2, \dots, y_n]^\top$ is the vector of observed values.
    \item $K$ is the kernel matrix, $K_{ij} = k(X_i, X_j)$.
    \item $k_* = [k(X_*, X_1), k(X_*, X_2), \dots, k(X_*, X_n)]^\top$ is the covariance vector between the new input point and the training data.
    \item $I$ is the identity matrix.
\end{itemize}

\subsection{Kernel Function Selection and Parameter Tuning}

Choosing an appropriate kernel function $k(X, X')$ is crucial for the effectiveness of Gaussian Process Regression. Several kernel functions were considered:

\paragraph{Radial Basis Function (RBF) Kernel / Gaussian Kernel:}

\[
k_{\text{RBF}}(X, X') = \sigma_f^2 \exp\left( -\frac{\| X - X' \|^2}{2 l^2} \right)
\]

\textbf{Reason:} The RBF kernel is smooth and infinitely differentiable, making it suitable for predicting smoothly varying functions. It is one of the most commonly used kernels.

\paragraph{Matern Kernel (e.g., $\nu = \frac{3}{2}$ or $\nu = \frac{5}{2}$):}

\[
k_{\text{Matern}}(X, X') = \sigma_f^2 \frac{2^{1 - \nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} \| X - X' \|}{l} \right)^\nu K_\nu \left( \frac{\sqrt{2\nu} \| X - X' \|}{l} \right)
\]

where $K_\nu$ is the modified Bessel function.

\textbf{Reason:} The Matern kernel is more flexible than the RBF kernel, allowing control over the smoothness of the function, making it suitable for handling data with varying smoothness.

\paragraph{Rational Quadratic Kernel:}

\[
k_{\text{RQ}}(X, X') = \sigma_f^2 \left(1 + \frac{\| X - X' \|^2}{2 \alpha l^2} \right)^{-\alpha}
\]

\textbf{Reason:} The Rational Quadratic kernel is a weighted sum of RBF kernels with different length scales, making it suitable for data with multi-scale features.

\subsubsection{Parameter Tuning}

The hyperparameters of the kernel functions were adjusted as follows:

\begin{itemize}
    \item \textbf{Length Scale $l$:} Values tested: $l = 0.1, 1, 10$. \\
    \textbf{Reason:} The length scale controls the rate of change of the function. Smaller $l$ values allow greater changes over shorter distances, suitable for capturing local features; larger $l$ values result in smoother functions.
    
    \item \textbf{Signal Variance $\sigma_f^2$:} Values tested: $\sigma_f^2 = 1, 5, 10$. \\
    \textbf{Reason:} The signal variance determines the amplitude of function variation. Adjusting $\sigma_f^2$ helps match the overall variability level of the data.
    
    \item \textbf{Noise Variance $\sigma_n^2$:} Values tested: $\sigma_n^2 = 0.01, 0.1, 1$. \\
    \textbf{Reason:} The noise variance reflects the level of noise in the observed data. Adjusting $\sigma_n^2$ according to the actual data helps prevent overfitting or underfitting.
\end{itemize}

\paragraph{Combining Kernels:}

Multiple kernels were also combined, for example, a linear kernel plus an RBF kernel:

\[
k(X, X') = k_{\text{Linear}}(X, X') + k_{\text{RBF}}(X, X')
\]

\textbf{Reason:} Combined kernels can capture different features in the data, such as global trends and local variations.

\subsection{Results Evaluation Methods}

To evaluate the performance of the GPR models with different kernel functions, several metrics were used:

\paragraph{Mean Squared Error (MSE):}

\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \mu_i)^2
\]

\textbf{Purpose:} Measures the average squared difference between the predicted and actual values. The smaller the MSE, the better the model performance.

\paragraph{Root Mean Squared Error (RMSE):}

\[
\text{RMSE} = \sqrt{\text{MSE}}
\]

\textbf{Purpose:} RMSE is in the same units as the original data, making it easier to interpret the magnitude of the error.

\paragraph{Mean Absolute Error (MAE):}

\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \mu_i|
\]

\textbf{Purpose:} Measures the average absolute difference between the predicted and actual values, less sensitive to outliers than MSE.

\paragraph{R-squared (Coefficient of Determination):}

\[
R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \mu_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}
\]

\textbf{Purpose:} Evaluates the proportion of variance in the dependent variable that is predictable from the independent variables. The closer $R^2$ is to 1, the better the model fits the data.

\subsection{Experimental Results and Analysis}

The results for different kernel functions are summarized in Table~\ref{tab:results}. These results indicate the performance of each kernel function in terms of MSE, RMSE, MAE, and $R^2$ score.

\begin{table}[h]
\centering
\caption{Performance of Different Kernels}
\label{tab:results}
\begin{tabular}{lcccc}
\hline
\textbf{Kernel} & \textbf{MSE} & \textbf{RMSE} & \textbf{MAE} & \textbf{$R^2$} \\
\hline
RBF & 0.6929 & 0.8324 & 0.6692 & 0.8764 \\
Matern ($\nu=1.5$) & 0.3211 & 0.5666 & 0.4659 & 0.9427 \\
Matern ($\nu=2.5$) & 0.4152 & 0.6443 & 0.5265 & 0.9259 \\
Rational Quadratic & \textbf{0.3199} & \textbf{0.5656} & \textbf{0.4639} & \textbf{0.9429} \\
\hline
\end{tabular}
\end{table}

\paragraph{Discussion:}

\begin{itemize}
    \item The \textbf{RBF kernel} showed relatively poor performance with the highest MSE and RMSE, indicating it might not be flexible enough to handle the non-smooth data in this case.
    \item The \textbf{Matern kernel ($\nu=1.5$)} performed very well with a low MSE of 0.3211 and a high $R^2$ of 0.9427, suggesting a good balance between model complexity and generalization ability.
    \item The \textbf{Matern kernel ($\nu=2.5$)} also performed well, but slightly worse than $\nu=1.5$, indicating its smoothness might not fully match the data characteristics.
    \item The \textbf{Rational Quadratic kernel} achieved the best performance with the lowest MSE (0.3199), RMSE (0.5656), and MAE (0.4639), indicating it is very well-suited to the multi-scale characteristics of the data.
\end{itemize}

\subsection{Conclusion}

Based on the experimental results, the \textbf{Rational Quadratic kernel} is selected as the optimal kernel function for the Gaussian Process Regression model. It shows the best fitting performance in terms of MSE, RMSE, and MAE metrics. The \textbf{Matern kernel ($\nu=1.5$)} is also a recommended choice when a slightly smoother function is needed.



\end{document}
